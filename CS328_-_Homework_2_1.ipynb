{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "### CS328 — Numerical Methods for Visual Computing\n",
    "- - -\n",
    "\n",
    "**Out** on Friday 21.10, **due** on Friday 4.11.\n",
    "\n",
    "This notebook contains literate code, i.e. brief fragments of Python surrounded by descriptive text. Please use the same format when submitting your answers. Begin your response to each problem with a <tt>&nbsp;<b>## Solution</b>&nbsp;&nbsp;</tt> markdown cell. Since this exercise includes a number of supplementary discussions, questions are explicitly marked with a **TODO** marker.\n",
    "\n",
    "<br><div class=\"alert alert-warning\">\n",
    "Please keep in mind that homework assignments must be done individually.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem -1: Warmup (not graded)\n",
    "$$\n",
    "\\newcommand{\\vb}{\\mathbf{b}}\n",
    "\\newcommand{\\vc}{\\mathbf{c}}\n",
    "\\newcommand{\\vx}{\\mathbf{x}}\n",
    "\\newcommand{\\mA}{\\mathbf{A}}\n",
    "\\newcommand{\\mL}{\\mathbf{L}}\n",
    "\\newcommand{\\mU}{\\mathbf{U}}\n",
    "\\newcommand{\\mP}{\\mathbf{P}}\n",
    "\\newcommand{\\mI}{\\mathbf{I}}\n",
    "$$\n",
    "\n",
    "The following Yes/No and paper & pencil questions are meant to check your comprehension of lecture and reading material in addition to linear algebra prerequisites.\n",
    "\n",
    "1. Recall that a permutation matrix $\\mP$ has exactly one entry with value $1$ in each row and column and zeros elsewhere. Prove that $\\mP^{-1}=\\mP^T$. You'll need the definitions of matrix multiplication and the matrix inverse, namely that $\\mA\\mA^{-1}=\\mI$. (where $\\mI$ is the identity matrix)<br><br>\n",
    "\n",
    "2. Which of the following parametric models can be fit to data using linear regression techniques?\n",
    "$$\n",
    "\\begin{align*}\n",
    "   f_1(x, \\vc) &= \\vc_0 + \\vc_1 x^2 + \\vc_2 \\sin(x) - \\sqrt{\\vc_3}\\\\\n",
    "   f_2(x, \\vc) &= \\vc_0^3 x^3 - x\\\\\n",
    "   f_3(x, \\vc) &= \\exp(\\vc_0 x)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "   here, $x$ represents the independent variable and $\\vc$ is a vector containing the model parameters.<br><br>\n",
    "\n",
    "3. A matrix $\\mA\\in\\mathbb{R}^{n\\times n}$ is said to be *symmetric and positive definite* (SPD) iff $\\mA=\\mA^T$ and $\\vx^T\\mA\\vx>0$ for all $\\vx\\in\\mathbb{R}^n$ with $\\vx\\ne \\mathbf{0}$. It's useful to know whether matrices are SPD, since specialized storage representations (don't store entries below the diagonal) and factorization algorithms (Cholesky decomposition) can exploit these properties, reducing the expense of linear system solving by approximately 50%.\n",
    "\n",
    "   Now recall the definition of the normal equations corresponding to the least squares system $\\mA\\vx\\approx\\vb$:\n",
    "$$\n",
    "\\mA^T\\mA\\vx=\\mA^T\\vb.\n",
    "$$\n",
    "  Assuming that we'd like solve a least squares problem with this approach, it would naturally be useful to know whether the product matrix $\\mA^T\\mA$ is symmetric and positive definite so that these more efficient algorithms can be used. Can you prove that $\\mA^T\\mA$ is SPD? (you'll likely need to introduce some extra assumptions on $\\mA$ for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Prelude\n",
    "\n",
    "As in the last assignment we'll begin by importing essential NumPy/SciPy/Matplotlib components that are needed to complete the exercises. The first two lines instruct Matplotlib to embed figures directly into the notebook and render them in sufficient quality for modern high-DPI (\"retina\") displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get to run numerical algorithms on high resolution images in this homework, which will involve a fair amount of plotting and visualization of intermediate results. The Matplotlib default settings unfortunately cause figures to be shown at a tiny resolution, so this next line changes the settings to make all figures large by default. The second line ensures low resolution images are rendered with sharp pixel contours instead of a blurry approximation, which will be helpful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Fitting a function to data, a.k.a. Regression Analysis (15 points)\n",
    "\n",
    "The following plot visualizes the total population of the Canton Vaud over a period of 35 years from 1979 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.linspace(1979, 2014, 36)\n",
    "population = np.array([128808, 128571, 128525, 128165, 128199, 127952, 127128, 126050, 126475, 126976, \n",
    "                       126699, 127515, 127118, 126058, 125457, 125264, 124561, 123577, 123325, 124205, \n",
    "                       125174, 124835, 125485, 126441, 126825, 127194, 127597, 128228, 129268, 130721, \n",
    "                       133265, 134753, 136288, 137586, 139390, 140228], dtype=np.float64)\n",
    "plt.plot(years, population, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will apply the tools of linear regression analysis and and linear least squares to fit an approximate parametric model to this dataset.\n",
    "\n",
    "1. **TODO**: Solve a linear system via the LU factorization (via ``la.lu_factor()`` and ``la.lu_solve()`` similar to what was shown in class) to fit a degree-35 polynomial to this dataset, which maps from years to population values. Does it reproduce the trend of the data when plotted at a finer resolution with 300 evaluations between 1979 and 2014? If so, please provide a visualization. Otherwise, please explain why it does not.<br><br>\n",
    "\n",
    "2. **TODO**: Now fit a degree-3 polynomial using the normal equations and create a similar plot. Does it reproduce the trend of the data when plotted at a finer resolution with 300 evaluations between 1979 and 2014?<br><br>\n",
    "\n",
    "3. **TODO**: Finally, solve the same least squares system once more, but using the economy size QR factorization (and *without* the use of the normal equations). The functions to use are ``la.qr(matrix, mode='economic')`` and ``la.solve_triangular()``. Which of the methods (2. or 3.) produces the solution with a lower residual?<br><br>\n",
    "\n",
    "4. **TODO**: What is the condition number of the product matrix $\\mathbf{A}^T\\mathbf{A}$ created by the normal equation approach in subproblem 2? (hint: use ``np.linalg.cond()``). Relate this information to your observations in subproblem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Solving LU system\n",
    "dates_matrix = np.column_stack([years**i for i in range(36)])\n",
    "L,U = la.lu_factor(dates_matrix)\n",
    "map_matrix = la.lu_solve((L,U), population, check_finite=False)\n",
    "\n",
    "#create new linear space and corresponding values\n",
    "dates_linear = np.linspace(1979, 2014, 300)\n",
    "datesBig_matrix = np.column_stack([dates_linear**j for j in range (36)])\n",
    "\n",
    "##plotting\n",
    "plt.plot(years, population, 'o')\n",
    "plt.plot(dates_linear, datesBig_matrix @ map_matrix);\n",
    "plt.title('Degree-35 polynomial')\n",
    "plt.xlabel('Date (year)')\n",
    "plt.ylabel('Canton de Vaud\\' population (people)')\n",
    "\n",
    "##printint and disscussion\n",
    "plt.show();\n",
    "print(\"Clearly, a degree-35 polynomial is faaar to big and creates these spikes. Every imprecision is\\n\"\n",
    "     \"accentuated, leading to this strange graph.\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's consider a Ax = b system, with x the unknown vector.\n",
    "A =  np.column_stack([years**i for i in range(7)])\n",
    "AtA = A.T @ A\n",
    "Atb = A.T @ population\n",
    "L,U = la.lu_factor(AtA)\n",
    "x_LU = la.lu_solve((L,U), Atb, check_finite=False)\n",
    "\n",
    "\n",
    "#dates_linear = np.linspace(1979, 2014, 300) #already computed earlier. Here to improve readibility.\n",
    "dates300_power7 = np.column_stack([dates_linear**j for j in range (7)])\n",
    "\n",
    "##Plotting\n",
    "plt.plot(years, population, 'o')\n",
    "plt.plot(dates_linear, dates300_power7 @ x_LU)\n",
    "plt.title('Degre-6 polynomial with LU factorization')\n",
    "plt.xlabel('Date (year)')\n",
    "plt.ylabel('Vaud\\' population (people)')\n",
    "\n",
    "plt.show();\n",
    "print(\"As it looks, the degree-6 polynomial (about as well as the degree-3 and degree-5) represents correctly the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same, we solve Ax = b system with QR\n",
    "A = np.column_stack([years**i for i in range(7)])\n",
    "Q, R = la.qr(A, mode='economic')\n",
    "x_QR = la.solve_triangular(R, Q.T @ population) #this solves x for Rx = (Q.T)(b) and R triangular\n",
    "\n",
    "#Plotting\n",
    "plt.plot(years, population, 'o')\n",
    "plt.plot(dates_linear, dates300_power7@ x_QR)\n",
    "plt.title('Degree-6 polynomial with QR factorization')\n",
    "plt.xlabel('Date (years)')\n",
    "plt.ylabel('Vaud\\'s population (people)')\n",
    "#show and discussion\n",
    "plt.show();\n",
    "print(\"We here see that the graph is slightly different (e.g. between 1980 and 1985). This is due to the solving model\\n\"\n",
    "     \"but it is not significant enough to influence the general look.\")\n",
    "residual2 =  (A @ x_LU) - population\n",
    "residual3 =  (A @ x_QR) - population\n",
    "resid_norm2 = la.norm(residual2)\n",
    "resid_norm3 = la.norm(residual3)\n",
    "print(\"The norms of residuals are respectively\", resid_norm2, \"and\", resid_norm3, \n",
    "      \",\\nmeaning the QR factorization has the lowest residue. A lower residue indicates a solution closer to reality.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##subpart 4\n",
    "print(\"Finally, the condition number of the A^T A matrix is\", np.linalg.cond(AtA),\n",
    "     \"\\nAs we know, the condition number of a matrix is 1 if it is an identity matrix, and infinity if\\n\"\n",
    "     \"the matrix is singular (det = 0). That clearly indicates that the matrix is almost singular (i.e. determinant is almost null)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Developing a raw photograph (85 points)\n",
    "\n",
    "Virtually any computer device in existence today is equipped with at least one digital camera; with over 5 billion users of digital photography, this technology has now fully permeated our society. In this exercise, you will get to build your own image processing pipeline that applies color space transformations to turn the raw data measured by a digital camera into a usable image that can be viewed on a computer screen. In the following, we will explain what this means, and how to implement the associated steps using tools from numerical linear algebra. But first, we shall begin with a brief review of the process that takes place when taking a picture.\n",
    "\n",
    "A digital camera uses an assembly of optical elements to focus the incident light onto a silicon sensor that consists of millions of tiny regions arranged in a regular grid.  The silicon is sensitive to light, and each small region on the sensor (generally referred to as a *pixel*) measures the portion of light that falls on it. \n",
    "<img width=\"400\" src=\"//rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/20/ccdcamera.png\">\n",
    "\n",
    "When no picture is being taken, the sensor is either inactive or used for a live preview (as in mobile phones or smaller cameras). When the actual photograph is taken, it is important to be able to control the precise amount of time during which  light is collected by the sensor. This is realized by means of the *shutter*, a small mechanical or electronic barrier that prevents light from reaching the sensor. The shutter can open and close very quickly, in about 1/4000th of a second.\n",
    "\n",
    "When taking a picture, the camera perform a sequence of steps in rapid succession: first, the shutter opens, allowing light to reach the pixels on the sensor. For a brief duration (known as the *exposure time*), the sensor collects all light that reaches the surface.  To be able to distinguish colors, each pixel on the sensor is covered with a tiny filter that will only permit certain wavelengths of light to pass through.\n",
    "<img width=\"400\" src=\"//rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/20/photon_rain.png\">\n",
    "Due its physical nature, light reaches the sensor in discrete amounts not unlike droplets of rain that fill a number of buckets with water. When the exposure time is very short, each pixel may only have received a few droplets, and the image is very noisy. When the exposure time is too long, the buckets will fill up completely, and it is impossible to recover a usable image. Once the exposure time has elapsed, the shutter closes so that the measurement ceases to change any further.\n",
    "\n",
    "Once the measurement is frozen, the camera proceeds to performs a full readout of the data associated with each pixel (i.e. the \"fill height\" of each bucket in the illustration above). This data is known as a *raw image*: a representation of the original sensor data without any additional processing. Raw images play a similar role as negatives in traditional photography: they contain all of the information that is needed to eventually create an image, but they aren't yet usable as an image on their own. Converting a raw image into an actual image entails decoding the information on the sensor and translating this data into displayable red/green/blue intensities that \"make sense\" on a computer screen, and which reproduce the colors that a human would have observed in the moment when the photo was taken.\n",
    "\n",
    "Analogous to classical photography, the process of turning the raw sensor data into a usable image is referred to as \"developing\" the image. Cell phone cameras generally perform this step automatically without user intervention. On the other hand, most professional cameras allow the user to choose between developing the image automatically within the camera or storing undeveloped raw images that can be manually developed by the user later on. The latter has become extremely popular, since raw image contain more information than would be available in a fully processed image (such as a JPEG image). For instance, JPEG files may not be able to represent the full range of colors perceived by the camera's sensor, and the lossy compression implies that there is some loss of image quality. A range of commercial (Adobe Photoshop, Lightroom, ..) and free (RawTherapee, darktable, ..) software tools can be used to develop raw image with full artistic control over the output.\n",
    "\n",
    "Developing an image can be an arbitrarily complex process, though the simplest version can be reduced to only four steps:\n",
    "\n",
    "1. Loading the raw image data\n",
    "2. Demosaicing\n",
    "3. Color space transformation\n",
    "4. Gamma correction\n",
    "\n",
    "We will now walk through each of these steps (slightly out of order):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the raw image data\n",
    "\n",
    "Begin by downloading the following file (~72 megabytes), which contains the raw sensor data of two photographs taken with a [Canon 6D](https://en.wikipedia.org/wiki/Canon_EOS_6D) camera: <a href=\"http://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/20/lacleman_raw.npz\"><tt>http://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/20/lacleman_raw.npz</tt></a>.\n",
    "\n",
    "Place the file into the same directory as the <tt>CS328 - Homework 2.ipynb</tt> file so that it is easy to load from Python. The following set of commands then import the file and extract all relevant data (this will take a few seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"lacleman_raw.npz\")\n",
    "tree = data['tree']\n",
    "whitebal = data['whitebal']\n",
    "patches_target = data['patches_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains three 2D NumPy arrays—two high resolution photographs (``tree`` and ``whitebal``) with approximately 20 million pixels each, and a mysterious small array (``patches_target``) that we'll need later on. We will only focus on the ``tree`` array for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.shape)\n",
    "print(whitebal.shape)\n",
    "print(patches_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Demosaicing (15 points)\n",
    "\n",
    "Let's try to display one of the raw images using Matplotlib!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tree);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is just a raw grid of sensor data without explicit color information, hence Matplotlib decided to use a heat color map to visualize the array contents. Purple, blue, green, and yellow colors correspond to pixel values of increasing magnitude. A strange grid-like artifact should be apparent when inspecting the image more carefully.\n",
    "\n",
    "**TODO** Do the following to zoom into a smaller part of the image: plot the image region between rows ``1000...1100`` and columns ``1000...1100`` (hint: use NumPy array slicing). Make use of the color map (``cmap``) parameter of ``imshow`` to ensure that Matplotlib visualizes the image data using a grayscale (``gray``) color map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tree[1000:1100, 1000:1100], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very strange.. what is going on here? The pattern you see turns out to be related to the design of the sensor that was discussed earlier. Because it's impossible for a single pixel to distinguish among different colors, the sensor consists of a regular arrangement of pixels that are sensitive to red, green, and blue. This is known as a *Bayer grid*. Note that the sensor contains twice as many green pixels as red or blue pixels; the reason for this  physiology: the eyes of humans are more sensitive to green wavelengths of light, hence it makes sense to capture the associated color information at a higher resolution.\n",
    "\n",
    "<img width=\"800\" src=\"//rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/21/bayergrid_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this kind of representation is not particularly useful: to be able to print or display images, we'll generally want to have information about *all* colors at *every* pixel. The process of taking the partial information from the Bayer grid and converting it into a full color image is known as *demosaicing*. Very high quality demosaicing algorithms exist that do a great job at filling in the missing information, but they can be quite complicated to implement. \n",
    "\n",
    "Here, we will implement the most basic version of demosaicing possible: instead of trying to infer the missing pixel values to obtain a full-color image with 5494x3666 pixels, we generate an image at *half* resolution (2747x1833 pixels) by collapsing repeated groups of 4 monochromatic pixels into a single colored pixel. We'll simply ignore one of the green pixels as part of this step for simplicity.\n",
    "\n",
    "<img width=\"300\" src=\"//rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/21/bayer-extract.png\">\n",
    "\n",
    "**TODO**: Create a function ``demosaic(image)``, which takes a raw image (a ``5494x3666`` array) as input and performs the operation discussed above, returning a ``2747x1833`` 3-tensor (i.e. a matrix with a third dimension for color with the ordering ``R, G, B``). The easiest way to do this is by extracting the red, green, and blue channels into their own images and then using ``np.dstack`` to create the 3-tensor. The figure above depicts the Bayer grid arrangement used by the Canon 6D camera that took this image. You probably shouldn't need more than 5 lines of code for the demosaicing function.\n",
    "\n",
    "A general suggestion for this assignment: since we're working with fairly large images, it's important to use vectorized NumPy function—in particular, you should never explicitly need to loop over the pixels of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demosaic(raw):\n",
    "    blueimage = raw[1::2, 1::2]\n",
    "    redimage = raw[::2, ::2]\n",
    "    greenimage = raw[::2, 1::2]\n",
    "    tensor = np.dstack((redimage, greenimage, blueimage))\n",
    "    return(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the demosaicing function on the ``tree`` image and plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(demosaic(tree));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gamma correction (10 points)\n",
    "\n",
    "Assuming that you got all the color channels into the right places, the previous visualization should have resulted in a very dark image of a tree on our beautiful Geneva Lake with a strange blue-green tint. What's going on here?\n",
    "\n",
    "As discussed before, the color values in the raw image are analogous to the \"fill level\" of a bucket of water. There is a direct (linear) relationship between how much light a sensor pixel received and the magnitude of the corresponding entry in the raw image.\n",
    "\n",
    "Somewhat surprisingly, this way of representing image intensity values is not directly suitable for display on a modern computer monitor. Current displays assume a more complex (nonlinear) relationship between pixel values and the brightness that will be produced as a result. In other words: when doubling the value stored in a pixel, the actually visible pixel brightness will not generally double as a result. This nonlinear mapping is known as the *gamma curve* of the display and is defined as\n",
    "$$\n",
    "I = V^\\gamma\n",
    "$$\n",
    "where $I$ is the resulting pixel brightness, $V$ is the pixel's value, and $\\gamma$ is a device dependent constant—usually $\\gamma\\approx 2.2$.\n",
    "To be able to display a raw image properly, we'll want to go *the other way*, i.e. to find out which pixel value to set so that the display reproduces the brightness that was originally recorded on the sensor.\n",
    "\n",
    "**TODO**: Define a function ``invgamma(image)`` that takes an arbitrary image of intensities and *inverts* the gamma curve (i.e. it computes $V$ given $I$), returning an array of pixel values of the same size. Implement the function in such a way that negative values are mapped to zero and values greater than ``1.0`` are mapped to ``1.0`` (this will be useful to avoid some potential pitfalls later on). You probably shouldn't need much more than 5 lines of code to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def invgamma(image):\n",
    "    #the order of clip and the power is important here : the other one would \n",
    "    #create runtime errors at the vers last point\n",
    "    I_big = np.clip(image, 0, 1)\n",
    "    return I_big**(1/2.2)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Plot the ``tree`` image once more, but this time run it through both the ``demosaic`` *and* the ``invgamma`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "plt.imshow(invgamma(demosaic(tree)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 Color space transformations (5 points)\n",
    "\n",
    "Assuming a correct implementation of the previously discussed components, you should now see an overall brighter image of the tree with natural contrast. However, the colors of the tree still look \"weird\" (it's the same unnatural blue-green tint we observed before). Why does this happen?\n",
    "\n",
    "There is an infinite space of possible varieties when building screens or sensors that emit or measure certain colors of light. Any particular RGB screen or camera will need to commit to a specific set of emission colors or sensitivities, which conceptually defines the axes of a 3D color coordinate system that is associated with that device. Unfortunately, this means that the red, green and blue colors perceived by the camera's sensor are generally not the same colors that are produced by the corresponding phosphors or LEDs on your screen. In other words, a color (such as ``[0.0, 0.5, 0.5]``,  which could be a dark blue green) captured by one device will need to be transformed so that it truly reproduces that same color on another device.\n",
    "\n",
    "<img width=\"800\" src=\"//rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/21/colorcube.png\">\n",
    "\n",
    "The above discussion greatly simplifies some aspects of color theory, but this description shall be sufficient for our purposes here. The tool we'll use to transform colors is simple: matrix-vector multiplication! The following fragment defines a small helper function that applies a color space transformation to every pixel in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_colormatrix(image, matrix):\n",
    "    \"\"\"\n",
    "    ``image`` is assumed to be an RGB image represented\n",
    "    as a NumPy array with shape [width, height, 3]. \n",
    "    \n",
    "    ``matrix`` is a 3x3 color transformation matrix\n",
    "    that will be applied to the color values of each pixel\n",
    "    \"\"\"\n",
    "    return image @ matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Let's try using it! Define a $3\\times 3$ matrix that swaps the blue and red color channel (what should this matrix look like?). Use it to transform the colors of the ``tree`` image and visualize the result.\n",
    "\n",
    "Note that you need to be careful when applying color transformation matrices: the sequence is always\n",
    "\n",
    "1. Demosaic\n",
    "2. Color transformation\n",
    "3. Gamma correction\n",
    "\n",
    "In particular, be careful not to swap steps 2 and 3. It doesn't make sense to transform a (nonlinear) gamma representation of colors using (linear) matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap = np.array([[0, 0, 1],[0, 1, 0],[1, 0, 0]])\n",
    "image = invgamma(apply_colormatrix(demosaic(tree), swap))\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.2 Color checkers\n",
    "\n",
    "Without any additional knowledge, it's unclear what transformation should be applied to ensure correct color reproduction. In the context of photography, the way that this problem is generally solved is by taking a picture of a color checker, which looks something like this:\n",
    "<img src=\"//rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2016/10/21/mini_colorchecker_surface.jpg\">\n",
    "\n",
    "A color checker is a standardized piece of cardboard, usually with 24 squares of different colors. The patch colors are specially chosen to represent a range of colors found on man-made and natural materials such as human skin or plants.\n",
    "\n",
    "We define the following helper function, which plots a given set of color checker values in a 6x4 grid similar to the above image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colorchecker(patch_data, title = ''):\n",
    "    plt.figure(figsize=(6.0, 4.0))\n",
    "    plt.suptitle(title)\n",
    "    for i in range(4):\n",
    "        for j in range(6):\n",
    "            plt.subplot(4, 6, i*6+j+1)\n",
    "            plt.imshow(invgamma(patch_data[np.newaxis, np.newaxis, i*6+j, :]))\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the red, green, and blue pixel intensities that are needed to reproduce the color checker colors on modern computer screens are *known* constants. They are stored in the mysterious ``patch_data`` array we loaded earlier. Thus, we should be able to plug them into the ``plot_colorchecker`` function to generate an image that resembles the above photograph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colorchecker(patches_target, title='Reference color checker');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.3 Obtaining and visualizing all required information (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I took the photograph of the tree on Geneva Lake, I also made a second photograph of a color checker, which is stored in the ``whitebal`` array.\n",
    "\n",
    "**TODO**: Use the functions defined earlier to visualize a demosaiced & gamma-corrected portion of rows ``680..1150`` and columns ``1020:1730`` of the ``whitebal`` image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = invgamma(demosaic(whitebal))\n",
    "checker = corrected[680:1150, 1020:1730]\n",
    "plt.imshow(checker);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue, we'll need to have a way of of extracting the captured colors of the individual patches in the ``whitebal`` image. The following function might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patch(image, i, j):\n",
    "    \"\"\"\n",
    "    Extracts the central portion of the color patch ``(i, j)`` from ``image``\n",
    "    \n",
    "    ``image`` is assumed to be a demosaiced linear (i.e. *not* gamma-corrected)\n",
    "    version of the color checker image\n",
    " \n",
    "    ``i`` denotes the row of the color checker (from 0 to 3)\n",
    "\n",
    "    ``j`` denotes the column of the color checker (from 0 to 5)\n",
    "    \"\"\"\n",
    "\n",
    "    start = np.array([i,   j  ]) * 113 + np.array([690, 1040]) + np.array([30, 30])\n",
    "    end   = np.array([i+1, j+1]) * 113 + np.array([690, 1040]) - np.array([30, 30])\n",
    "\n",
    "    return image[start[0]:end[0], start[1]:end[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Write a piece of code, which extracts all 24 patch colors from the ``whitebal`` image and converts it into the same format as the reference colors stored in ``patches_target`` (i.e. a $24\\times 3$ matrix). Store the resulting information in an array named ``patches_camera``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = demosaic(whitebal)\n",
    "\n",
    "#we define a method that uses the same syntax as in 1.1. Creates a 1x3 array with color channels in the columns.\n",
    "def colorsCasesRGB(i, j):\n",
    "    return np.array([extract_patch(demo, i, j)[53//2][53//2][c] for c in range(3)]) #53//2 goes in the middle of the case\n",
    "\n",
    "#now we apply it on every line of the table\n",
    "patches_camera = np.array([colorsCasesRGB(i//6, i%6) for i in range(24)]) #// is an integer division when % is modulo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Apply the ``plot_colorchecker`` function to the ``patches_camera`` array generated above. The result should match the photograph from earlier (i.e. a weird color checker with a blue-green tint). Be careful that your extraction code does not change the ordering of patches (that much should be visible even with the distorted colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colorchecker(patches_camera, title='Custom color checker');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.4 Determining the correct color space transformation (40 points)\n",
    "\n",
    "Now begins the truly interesting part: we need to find a color space transformation that will allow us to convert red/green/blue values on the sensor to meaningful red/green/blue values on a modern display. We've previously taken an image of a color checker and thus know the value of each patch in the camera's RGB coordinate system (stored in ``patches_camera``). We also know, what value it *should* theoretically have in the display RGB coordinate system (``patches_target``).\n",
    "\n",
    "A suitable color space transformation $f$ should thus satisfy a set of equations of the form\n",
    "$$\n",
    "f(\\mathbf{c}_{\\mathrm{camera}, i}) = \\mathbf{c}_{\\mathrm{target}, i} \\qquad (i=1, \\dots, 24),\n",
    "$$\n",
    "where ``f`` applies a color-space transformation using a matrix-vector multiplication\n",
    "$$\n",
    "f(\\vx) = \\mA\\vx.\n",
    "$$\n",
    "\n",
    "Here, $\\mA$ is a $3\\times 3$ matrix, i.e. there are 9 unknowns. On the other hand, we have observed the RGB values of 24 different color patches, which means that there were 72 observations. Since there are more observations that unknowns, the problem does not have a solution in the traditional sense. However, perhaps we can find a solution such that\n",
    "$$\n",
    "f(\\mathbf{c}_{\\mathrm{camera}, i}) \\approx \\mathbf{c}_{\\mathrm{target}, i} \\qquad (i=1, \\dots, 24).\n",
    "$$\n",
    "\n",
    "This is a least squares problem!\n",
    "\n",
    "**TODO**: Convert the above set of equations into a concrete least squares system and solve it using any of the methods discussed in class.\n",
    "\n",
    "This is the hardest part of this problem. You'll want to carefully think about how to solve the problem on pencil and paper first before implementing it in Python. A few good topics to think about: where should the camera color observations go? Where do the target pixel values go? What dimensions should the system have? What kind of structure does the system matrix have? (A hint: it's not a full matrix -- some elements will be filled with zeros). It's unusual to have unknowns that are arranged as a matrix, but perhaps we can just think of them as a long vector with 9 entries?\n",
    "\n",
    "There are a two functions that could be helpful: ``np.ravel()`` rearranges the contents of a matrix into a tall column vector. ``np.reshape()`` goes the other way and permits rearranging a vector back into a matrix. \n",
    "\n",
    "A particularly short implementation (3 lines) is possible using the functions ``np.eye(N)`` (which returns an $N\\times N$ identity matrix) and ``np.kron()`` which computes a [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) -- a fancy name of the operation, which inserts scaled version of a matrix into another matrix. It's also perfectly fine to write a loop here, which just initializes the matrix elements individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_Big = np.zeros((72, 9), dtype=float) #just an empty canevas\n",
    "\n",
    "#This changes a 24x3 matrix into a 72x9 one, with the following shape :\n",
    "#[A,B,C]\n",
    "#[D,E,F]...becomes \n",
    "#[A,B,C,0,0,0,0,0,0]\n",
    "#[0,0,0,A,B,C,0,0,0]\n",
    "#[0,0,0,0,0,0,A,B,C]\n",
    "#[D,E,F,0,0,0,0,0,0]\n",
    "#[0,0,0,D,E,F,0,0,0]...\n",
    "for i in range(24):\n",
    "    for j in range(3):\n",
    "        A_Big[3*i][j] = patches_camera[i][j]\n",
    "        A_Big[3*i+1][j+3] = patches_camera[i][j]\n",
    "        A_Big[3*i+2][j+6] = patches_camera[i][j]\n",
    "\n",
    "#Now we can change our unknown matrix into a vector\n",
    "targetAsVec = np.ravel(patches_target)\n",
    "#And solve the system as we know, with QR facto\n",
    "Q, R = la.qr(A_Big, mode='economic')\n",
    "x= la.solve_triangular(R, Q.T @ targetAsVec)\n",
    "\n",
    "x_matrix = np.reshape(x, (3,3))\n",
    "\n",
    "#The point here is to modify the system 24x3 * 3x3 = 24x3 into a 72x9 * 9x1 = 72x9 one, solve it and then \n",
    "#go back to original shape. We quickly see that to have the same operations going on, we need to transform as in\n",
    "#the double for loop, meaning tripling the rows and columns and having this \"identity-like\" shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Apply your computed color space transformation to the color values in ``patches_camera`` and visualize the result using the ``plot_colorchecker`` function. The result should look virtually identical to the reference colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colorchecker(apply_colormatrix(patches_camera, x_matrix), 'Corrected custom color-checker');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Finally!! Now, apply the color space transformation to the ``tree`` image and plot the result.\n",
    "\n",
    "You have learned how digital photographs are developed and implemented an end-to-end pipeline that solves a least squares problem to obtain calibrated colors. Congratulations! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(invgamma(apply_colormatrix(demosaic(tree), x_matrix)));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
